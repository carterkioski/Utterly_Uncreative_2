{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from tensorflow import lite\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import categorical_accuracy, AUC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>binary_type</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000c1434d8d7</td>\n",
       "      <td>2</td>\n",
       "      <td>DR</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001639a390f0</td>\n",
       "      <td>4</td>\n",
       "      <td>DR</td>\n",
       "      <td>Proliferate_DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0024cdab0c1e</td>\n",
       "      <td>1</td>\n",
       "      <td>DR</td>\n",
       "      <td>Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002c21358ce6</td>\n",
       "      <td>0</td>\n",
       "      <td>No_DR</td>\n",
       "      <td>No_DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005b95c28852</td>\n",
       "      <td>0</td>\n",
       "      <td>No_DR</td>\n",
       "      <td>No_DR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis binary_type            type\n",
       "0  000c1434d8d7          2          DR        Moderate\n",
       "1  001639a390f0          4          DR  Proliferate_DR\n",
       "2  0024cdab0c1e          1          DR            Mild\n",
       "3  002c21358ce6          0       No_DR           No_DR\n",
       "4  005b95c28852          0       No_DR           No_DR"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add an additional column, mapping to the type\n",
    "Image_info_df = pd.read_csv('../Data/Gaussian_Retina_Data/train.csv')\n",
    "\n",
    "diagnosis_dict_binary = {\n",
    "    0: 'No_DR',\n",
    "    1: 'DR',\n",
    "    2: 'DR',\n",
    "    3: 'DR',\n",
    "    4: 'DR'\n",
    "}\n",
    "\n",
    "diagnosis_dict = {\n",
    "    0: 'No_DR',\n",
    "    1: 'Mild',\n",
    "    2: 'Moderate',\n",
    "    3: 'Severe',\n",
    "    4: 'Proliferate_DR',\n",
    "}\n",
    "\n",
    "\n",
    "Image_info_df['binary_type'] =  Image_info_df['diagnosis'].map(diagnosis_dict_binary.get)\n",
    "Image_info_df['type'] = Image_info_df['diagnosis'].map(diagnosis_dict.get)\n",
    "Image_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f4bca3c9e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show how many of each type of retinopathy\n",
    "Image_info_df['type'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f4bcb47588>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the balance between no retinopathy and stages of retinopathy\n",
    "Image_info_df['binary_type'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No_DR             1441\n",
      "Moderate           799\n",
      "Mild               301\n",
      "Proliferate_DR     239\n",
      "Severe             149\n",
      "Name: type, dtype: int64 \n",
      "\n",
      "No_DR             364\n",
      "Moderate          200\n",
      "Mild               69\n",
      "Proliferate_DR     56\n",
      "Severe             44\n",
      "Name: type, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split into train, and test sets\n",
    "train, test = train_test_split(Image_info_df, test_size = 0.20)\n",
    "\n",
    "# Show how many in each set\n",
    "print(train['type'].value_counts(), '\\n')\n",
    "print(test['type'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working directories for train/val/test\n",
    "base_dir = ''\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "if os.path.exists(base_dir):\n",
    "    shutil.rmtree(base_dir)\n",
    "\n",
    "if os.path.exists(train_dir):\n",
    "    shutil.rmtree(train_dir)\n",
    "os.makedirs(train_dir)\n",
    "\n",
    "if os.path.exists(test_dir):\n",
    "    shutil.rmtree(test_dir)\n",
    "os.makedirs(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy images to respective working directory\n",
    "src_dir = '../Data/Gaussian_Retina_Data/gaussian_filtered_images/gaussian_filtered_images/'\n",
    "for index, row in train.iterrows():\n",
    "    diagnosis = row['type']\n",
    "    binary_diagnosis = row['binary_type']\n",
    "    id_code = row['id_code'] + \".png\"\n",
    "    srcfile = os.path.join(src_dir, diagnosis, id_code)\n",
    "    dstfile = os.path.join(train_dir, diagnosis)\n",
    "    os.makedirs(dstfile, exist_ok = True)\n",
    "    shutil.copy(srcfile, dstfile)\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    diagnosis = row['type']\n",
    "    binary_diagnosis = row['binary_type']\n",
    "    id_code = row['id_code'] + \".png\"\n",
    "    srcfile = os.path.join(src_dir, diagnosis, id_code)\n",
    "    dstfile = os.path.join(test_dir, diagnosis)\n",
    "    os.makedirs(dstfile, exist_ok = True)\n",
    "    shutil.copy(srcfile, dstfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2929 images belonging to 5 classes.\n",
      "Found 733 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setting up ImageDataGenerator for train/val/test \n",
    "\n",
    "train_path = 'train'\n",
    "test_path = 'test'\n",
    "\n",
    "train_batches = ImageDataGenerator(rescale = 1./255).flow_from_directory(train_path, target_size=(224,224), shuffle = False)\n",
    "test_batches = ImageDataGenerator(rescale = 1./255).flow_from_directory(test_path, target_size=(224,224), shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Layer CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 112, 112, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 56, 56, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1, 1, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 1, 1, 128)         295040    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 1032      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 737,781\n",
      "Trainable params: 736,341\n",
      "Non-trainable params: 1,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), padding=\"same\", activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv2D(32, (3, 3), padding=\"same\", activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv2D(32, (3, 3), padding=\"same\", activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(1, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(8, activation = 'relu'),\n",
    "    layers.Dropout(0.15),\n",
    "    layers.Dense(5, activation = 'softmax')\n",
    "\n",
    "])\n",
    "\n",
    "# Show the structure of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "92/92 [==============================] - 122s 1s/step - loss: 0.7582 - acc: 0.1137 - val_loss: 0.6926 - val_acc: 0.0764\n",
      "Epoch 2/200\n",
      "92/92 [==============================] - 128s 1s/step - loss: 0.7235 - acc: 0.1144 - val_loss: 0.6847 - val_acc: 0.0764\n",
      "Epoch 3/200\n",
      "92/92 [==============================] - 140s 2s/step - loss: 0.7033 - acc: 0.1219 - val_loss: 0.6881 - val_acc: 0.0819\n",
      "Epoch 4/200\n",
      "92/92 [==============================] - 141s 2s/step - loss: 0.6878 - acc: 0.1250 - val_loss: 0.6991 - val_acc: 0.0982\n",
      "Epoch 5/200\n",
      "92/92 [==============================] - 130s 1s/step - loss: 0.6765 - acc: 0.1321 - val_loss: 0.7165 - val_acc: 0.1119\n",
      "Epoch 6/200\n",
      "92/92 [==============================] - 103s 1s/step - loss: 0.6672 - acc: 0.1287 - val_loss: 0.7142 - val_acc: 0.1364\n",
      "Epoch 7/200\n",
      "92/92 [==============================] - 98s 1s/step - loss: 0.6583 - acc: 0.1338 - val_loss: 0.7035 - val_acc: 0.1774\n",
      "Epoch 8/200\n",
      "92/92 [==============================] - 98s 1s/step - loss: 0.6526 - acc: 0.1393 - val_loss: 0.6945 - val_acc: 0.1787\n",
      "Epoch 9/200\n",
      "92/92 [==============================] - 97s 1s/step - loss: 0.6430 - acc: 0.1413 - val_loss: 0.6881 - val_acc: 0.1910\n",
      "Epoch 10/200\n",
      "92/92 [==============================] - 117s 1s/step - loss: 0.6365 - acc: 0.1492 - val_loss: 0.6819 - val_acc: 0.1692\n",
      "Epoch 11/200\n",
      "92/92 [==============================] - 115s 1s/step - loss: 0.6302 - acc: 0.1499 - val_loss: 0.6788 - val_acc: 0.1583\n",
      "Epoch 12/200\n",
      "92/92 [==============================] - 99s 1s/step - loss: 0.6283 - acc: 0.1502 - val_loss: 0.6772 - val_acc: 0.1869\n",
      "Epoch 13/200\n",
      "92/92 [==============================] - 107s 1s/step - loss: 0.6221 - acc: 0.1571 - val_loss: 0.6718 - val_acc: 0.1733\n",
      "Epoch 14/200\n",
      "92/92 [==============================] - 101s 1s/step - loss: 0.6165 - acc: 0.1642 - val_loss: 0.6705 - val_acc: 0.1814\n",
      "Epoch 15/200\n",
      "92/92 [==============================] - 101s 1s/step - loss: 0.6133 - acc: 0.1598 - val_loss: 0.6680 - val_acc: 0.1896\n",
      "Epoch 16/200\n",
      "92/92 [==============================] - 98s 1s/step - loss: 0.6100 - acc: 0.1717 - val_loss: 0.6655 - val_acc: 0.1965\n",
      "Epoch 17/200\n",
      "92/92 [==============================] - 94s 1s/step - loss: 0.6042 - acc: 0.1673 - val_loss: 0.6648 - val_acc: 0.1855\n",
      "Epoch 18/200\n",
      "92/92 [==============================] - 92s 998ms/step - loss: 0.6013 - acc: 0.1878 - val_loss: 0.6629 - val_acc: 0.1992\n",
      "Epoch 19/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5965 - acc: 0.1850 - val_loss: 0.6595 - val_acc: 0.1978\n",
      "Epoch 20/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5939 - acc: 0.1864 - val_loss: 0.6591 - val_acc: 0.2196\n",
      "Epoch 21/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5923 - acc: 0.1980 - val_loss: 0.6592 - val_acc: 0.2142\n",
      "Epoch 22/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5880 - acc: 0.1943 - val_loss: 0.6571 - val_acc: 0.2156\n",
      "Epoch 23/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5872 - acc: 0.1970 - val_loss: 0.6540 - val_acc: 0.2142\n",
      "Epoch 24/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5818 - acc: 0.2113 - val_loss: 0.6538 - val_acc: 0.2074\n",
      "Epoch 25/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5810 - acc: 0.2151 - val_loss: 0.6516 - val_acc: 0.2224\n",
      "Epoch 26/200\n",
      "92/92 [==============================] - 96s 1s/step - loss: 0.5766 - acc: 0.2202 - val_loss: 0.6499 - val_acc: 0.2142\n",
      "Epoch 27/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5734 - acc: 0.2366 - val_loss: 0.6482 - val_acc: 0.2224\n",
      "Epoch 28/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.5692 - acc: 0.2318 - val_loss: 0.6456 - val_acc: 0.2401\n",
      "Epoch 29/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.5715 - acc: 0.2417 - val_loss: 0.6482 - val_acc: 0.2510\n",
      "Epoch 30/200\n",
      "92/92 [==============================] - 92s 998ms/step - loss: 0.5664 - acc: 0.2434 - val_loss: 0.6438 - val_acc: 0.2442\n",
      "Epoch 31/200\n",
      "92/92 [==============================] - 92s 997ms/step - loss: 0.5655 - acc: 0.2462 - val_loss: 0.6427 - val_acc: 0.2374\n",
      "Epoch 32/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5618 - acc: 0.2571 - val_loss: 0.6407 - val_acc: 0.2497\n",
      "Epoch 33/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.5597 - acc: 0.2653 - val_loss: 0.6393 - val_acc: 0.2483\n",
      "Epoch 34/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.5593 - acc: 0.2677 - val_loss: 0.6379 - val_acc: 0.2565\n",
      "Epoch 35/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5591 - acc: 0.2724 - val_loss: 0.6368 - val_acc: 0.2756\n",
      "Epoch 36/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.5533 - acc: 0.2841 - val_loss: 0.6353 - val_acc: 0.2456\n",
      "Epoch 37/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5507 - acc: 0.2892 - val_loss: 0.6324 - val_acc: 0.2578\n",
      "Epoch 38/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5488 - acc: 0.2936 - val_loss: 0.6322 - val_acc: 0.2647\n",
      "Epoch 39/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.5468 - acc: 0.3028 - val_loss: 0.6305 - val_acc: 0.2701\n",
      "Epoch 40/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5440 - acc: 0.3011 - val_loss: 0.6283 - val_acc: 0.2810\n",
      "Epoch 41/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.5442 - acc: 0.2998 - val_loss: 0.6307 - val_acc: 0.2756\n",
      "Epoch 42/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.5395 - acc: 0.3138 - val_loss: 0.6270 - val_acc: 0.2906\n",
      "Epoch 43/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5416 - acc: 0.3127 - val_loss: 0.6260 - val_acc: 0.2879\n",
      "Epoch 44/200\n",
      "92/92 [==============================] - 92s 998ms/step - loss: 0.5397 - acc: 0.3134 - val_loss: 0.6250 - val_acc: 0.2879\n",
      "Epoch 45/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5371 - acc: 0.3243 - val_loss: 0.6243 - val_acc: 0.2920\n",
      "Epoch 46/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5348 - acc: 0.3298 - val_loss: 0.6223 - val_acc: 0.3083\n",
      "Epoch 47/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5317 - acc: 0.3353 - val_loss: 0.6232 - val_acc: 0.2933\n",
      "Epoch 48/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5320 - acc: 0.3353 - val_loss: 0.6178 - val_acc: 0.2988\n",
      "Epoch 49/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.5298 - acc: 0.3462 - val_loss: 0.6186 - val_acc: 0.3233\n",
      "Epoch 50/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5288 - acc: 0.3540 - val_loss: 0.6186 - val_acc: 0.3083\n",
      "Epoch 51/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5279 - acc: 0.3441 - val_loss: 0.6159 - val_acc: 0.3151\n",
      "Epoch 52/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5236 - acc: 0.3558 - val_loss: 0.6154 - val_acc: 0.3233\n",
      "Epoch 53/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.5200 - acc: 0.3592 - val_loss: 0.6154 - val_acc: 0.3179\n",
      "Epoch 54/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5209 - acc: 0.3667 - val_loss: 0.6118 - val_acc: 0.3329\n",
      "Epoch 55/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5196 - acc: 0.3704 - val_loss: 0.6131 - val_acc: 0.3274\n",
      "Epoch 56/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5186 - acc: 0.3742 - val_loss: 0.6110 - val_acc: 0.3302\n",
      "Epoch 57/200\n",
      "92/92 [==============================] - 92s 998ms/step - loss: 0.5180 - acc: 0.3715 - val_loss: 0.6089 - val_acc: 0.3315\n",
      "Epoch 58/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5118 - acc: 0.3882 - val_loss: 0.6060 - val_acc: 0.3342\n",
      "Epoch 59/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5108 - acc: 0.3926 - val_loss: 0.6039 - val_acc: 0.3383\n",
      "Epoch 60/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5112 - acc: 0.3902 - val_loss: 0.6039 - val_acc: 0.3465\n",
      "Epoch 61/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5065 - acc: 0.4083 - val_loss: 0.6047 - val_acc: 0.3438\n",
      "Epoch 62/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5070 - acc: 0.4114 - val_loss: 0.6005 - val_acc: 0.3615\n",
      "Epoch 63/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5031 - acc: 0.4135 - val_loss: 0.5993 - val_acc: 0.3547\n",
      "Epoch 64/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.5035 - acc: 0.4080 - val_loss: 0.5990 - val_acc: 0.3602\n",
      "Epoch 65/200\n",
      "92/92 [==============================] - 96s 1s/step - loss: 0.5025 - acc: 0.4053 - val_loss: 0.6000 - val_acc: 0.3506\n",
      "Epoch 66/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.5024 - acc: 0.4070 - val_loss: 0.5946 - val_acc: 0.3561\n",
      "Epoch 67/200\n",
      "92/92 [==============================] - 94s 1s/step - loss: 0.4989 - acc: 0.4196 - val_loss: 0.5934 - val_acc: 0.3602\n",
      "Epoch 68/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4957 - acc: 0.4165 - val_loss: 0.5982 - val_acc: 0.3806\n",
      "Epoch 69/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.4964 - acc: 0.4295 - val_loss: 0.5944 - val_acc: 0.3656\n",
      "Epoch 70/200\n",
      "92/92 [==============================] - 92s 998ms/step - loss: 0.4952 - acc: 0.4322 - val_loss: 0.5929 - val_acc: 0.3683\n",
      "Epoch 71/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4910 - acc: 0.4387 - val_loss: 0.5908 - val_acc: 0.3683\n",
      "Epoch 72/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4941 - acc: 0.4353 - val_loss: 0.5896 - val_acc: 0.3779\n",
      "Epoch 73/200\n",
      "92/92 [==============================] - 96s 1s/step - loss: 0.4886 - acc: 0.4377 - val_loss: 0.5859 - val_acc: 0.3806\n",
      "Epoch 74/200\n",
      "92/92 [==============================] - 94s 1s/step - loss: 0.4873 - acc: 0.4336 - val_loss: 0.5873 - val_acc: 0.3711\n",
      "Epoch 75/200\n",
      "92/92 [==============================] - 94s 1s/step - loss: 0.4863 - acc: 0.4380 - val_loss: 0.5889 - val_acc: 0.3834\n",
      "Epoch 76/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4863 - acc: 0.4285 - val_loss: 0.5865 - val_acc: 0.3861\n",
      "Epoch 77/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4815 - acc: 0.4595 - val_loss: 0.5841 - val_acc: 0.3915\n",
      "Epoch 78/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4786 - acc: 0.4510 - val_loss: 0.5851 - val_acc: 0.3902\n",
      "Epoch 79/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4828 - acc: 0.4531 - val_loss: 0.5827 - val_acc: 0.3956\n",
      "Epoch 80/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4771 - acc: 0.4507 - val_loss: 0.5840 - val_acc: 0.3902\n",
      "Epoch 81/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4767 - acc: 0.4565 - val_loss: 0.5792 - val_acc: 0.3956\n",
      "Epoch 82/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4745 - acc: 0.4691 - val_loss: 0.5778 - val_acc: 0.3956\n",
      "Epoch 83/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4724 - acc: 0.4660 - val_loss: 0.5782 - val_acc: 0.4052\n",
      "Epoch 84/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4731 - acc: 0.4657 - val_loss: 0.5745 - val_acc: 0.4052\n",
      "Epoch 85/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4712 - acc: 0.4756 - val_loss: 0.5745 - val_acc: 0.4025\n",
      "Epoch 86/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.4719 - acc: 0.4616 - val_loss: 0.5727 - val_acc: 0.4106\n",
      "Epoch 87/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4694 - acc: 0.4725 - val_loss: 0.5718 - val_acc: 0.4147\n",
      "Epoch 88/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4671 - acc: 0.4811 - val_loss: 0.5704 - val_acc: 0.4093\n",
      "Epoch 89/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4641 - acc: 0.4807 - val_loss: 0.5688 - val_acc: 0.4134\n",
      "Epoch 90/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4670 - acc: 0.4729 - val_loss: 0.5725 - val_acc: 0.4216\n",
      "Epoch 91/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4634 - acc: 0.4817 - val_loss: 0.5676 - val_acc: 0.4325\n",
      "Epoch 92/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.4636 - acc: 0.4742 - val_loss: 0.5671 - val_acc: 0.4229\n",
      "Epoch 93/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4600 - acc: 0.4817 - val_loss: 0.5638 - val_acc: 0.4243\n",
      "Epoch 94/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4584 - acc: 0.4879 - val_loss: 0.5656 - val_acc: 0.4325\n",
      "Epoch 95/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4523 - acc: 0.4985 - val_loss: 0.5614 - val_acc: 0.4243\n",
      "Epoch 96/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4577 - acc: 0.4869 - val_loss: 0.5603 - val_acc: 0.4188\n",
      "Epoch 97/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4541 - acc: 0.4865 - val_loss: 0.5580 - val_acc: 0.4325\n",
      "Epoch 98/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4521 - acc: 0.4882 - val_loss: 0.5591 - val_acc: 0.4256\n",
      "Epoch 99/200\n",
      "92/92 [==============================] - 94s 1s/step - loss: 0.4525 - acc: 0.4920 - val_loss: 0.5568 - val_acc: 0.4352\n",
      "Epoch 100/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4478 - acc: 0.4991 - val_loss: 0.5600 - val_acc: 0.4270\n",
      "Epoch 101/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4477 - acc: 0.4998 - val_loss: 0.5574 - val_acc: 0.4434\n",
      "Epoch 102/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.4464 - acc: 0.5012 - val_loss: 0.5554 - val_acc: 0.4338\n",
      "Epoch 103/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.4463 - acc: 0.5050 - val_loss: 0.5547 - val_acc: 0.4297\n",
      "Epoch 104/200\n",
      "92/92 [==============================] - 96s 1s/step - loss: 0.4473 - acc: 0.4869 - val_loss: 0.5542 - val_acc: 0.4270\n",
      "Epoch 105/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4433 - acc: 0.5053 - val_loss: 0.5503 - val_acc: 0.4475\n",
      "Epoch 106/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4399 - acc: 0.5101 - val_loss: 0.5508 - val_acc: 0.4407\n",
      "Epoch 107/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4419 - acc: 0.5073 - val_loss: 0.5509 - val_acc: 0.4461\n",
      "Epoch 108/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4423 - acc: 0.5060 - val_loss: 0.5477 - val_acc: 0.4516\n",
      "Epoch 109/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4388 - acc: 0.5002 - val_loss: 0.5488 - val_acc: 0.4420\n",
      "Epoch 110/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4334 - acc: 0.5155 - val_loss: 0.5467 - val_acc: 0.4570\n",
      "Epoch 111/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4299 - acc: 0.5227 - val_loss: 0.5442 - val_acc: 0.4570\n",
      "Epoch 112/200\n",
      "92/92 [==============================] - 94s 1s/step - loss: 0.4339 - acc: 0.5114 - val_loss: 0.5442 - val_acc: 0.4475\n",
      "Epoch 113/200\n",
      "92/92 [==============================] - 96s 1s/step - loss: 0.4334 - acc: 0.5207 - val_loss: 0.5432 - val_acc: 0.4516\n",
      "Epoch 114/200\n",
      "92/92 [==============================] - 98s 1s/step - loss: 0.4314 - acc: 0.5152 - val_loss: 0.5478 - val_acc: 0.4557\n",
      "Epoch 115/200\n",
      "92/92 [==============================] - 97s 1s/step - loss: 0.4303 - acc: 0.5251 - val_loss: 0.5423 - val_acc: 0.4543\n",
      "Epoch 116/200\n",
      "92/92 [==============================] - 95s 1s/step - loss: 0.4288 - acc: 0.5295 - val_loss: 0.5435 - val_acc: 0.4570\n",
      "Epoch 117/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4259 - acc: 0.5333 - val_loss: 0.5397 - val_acc: 0.4447\n",
      "Epoch 118/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4274 - acc: 0.5302 - val_loss: 0.5388 - val_acc: 0.4502\n",
      "Epoch 119/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4217 - acc: 0.5357 - val_loss: 0.5349 - val_acc: 0.4652\n",
      "Epoch 120/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4212 - acc: 0.5282 - val_loss: 0.5380 - val_acc: 0.4516\n",
      "Epoch 121/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4198 - acc: 0.5370 - val_loss: 0.5368 - val_acc: 0.4598\n",
      "Epoch 122/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.4210 - acc: 0.5394 - val_loss: 0.5377 - val_acc: 0.4584\n",
      "Epoch 123/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4136 - acc: 0.5456 - val_loss: 0.5323 - val_acc: 0.4652\n",
      "Epoch 124/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4123 - acc: 0.5490 - val_loss: 0.5345 - val_acc: 0.4543\n",
      "Epoch 125/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4142 - acc: 0.5394 - val_loss: 0.5300 - val_acc: 0.4625\n",
      "Epoch 126/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4185 - acc: 0.5312 - val_loss: 0.5295 - val_acc: 0.4570\n",
      "Epoch 127/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.4131 - acc: 0.5435 - val_loss: 0.5296 - val_acc: 0.4693\n",
      "Epoch 128/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4112 - acc: 0.5401 - val_loss: 0.5283 - val_acc: 0.4720\n",
      "Epoch 129/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4114 - acc: 0.5442 - val_loss: 0.5252 - val_acc: 0.4638\n",
      "Epoch 130/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4106 - acc: 0.5394 - val_loss: 0.5219 - val_acc: 0.4775\n",
      "Epoch 131/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4078 - acc: 0.5493 - val_loss: 0.5288 - val_acc: 0.4748\n",
      "Epoch 132/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4011 - acc: 0.5606 - val_loss: 0.5230 - val_acc: 0.4761\n",
      "Epoch 133/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4073 - acc: 0.5476 - val_loss: 0.5214 - val_acc: 0.4789\n",
      "Epoch 134/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4018 - acc: 0.5565 - val_loss: 0.5211 - val_acc: 0.4679\n",
      "Epoch 135/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4023 - acc: 0.5517 - val_loss: 0.5180 - val_acc: 0.4789\n",
      "Epoch 136/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3982 - acc: 0.5633 - val_loss: 0.5243 - val_acc: 0.4884\n",
      "Epoch 137/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4003 - acc: 0.5555 - val_loss: 0.5243 - val_acc: 0.4898\n",
      "Epoch 138/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4006 - acc: 0.5626 - val_loss: 0.5191 - val_acc: 0.4829\n",
      "Epoch 139/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.4000 - acc: 0.5579 - val_loss: 0.5218 - val_acc: 0.4748\n",
      "Epoch 140/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3990 - acc: 0.5586 - val_loss: 0.5180 - val_acc: 0.4884\n",
      "Epoch 141/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3979 - acc: 0.5664 - val_loss: 0.5184 - val_acc: 0.4870\n",
      "Epoch 142/200\n",
      "92/92 [==============================] - 95s 1s/step - loss: 0.3934 - acc: 0.5691 - val_loss: 0.5168 - val_acc: 0.4829\n",
      "Epoch 143/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3935 - acc: 0.5603 - val_loss: 0.5191 - val_acc: 0.4884\n",
      "Epoch 144/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3942 - acc: 0.5719 - val_loss: 0.5095 - val_acc: 0.4884\n",
      "Epoch 145/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3908 - acc: 0.5726 - val_loss: 0.5178 - val_acc: 0.4857\n",
      "Epoch 146/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3840 - acc: 0.5845 - val_loss: 0.5133 - val_acc: 0.4898\n",
      "Epoch 147/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3919 - acc: 0.5640 - val_loss: 0.5134 - val_acc: 0.4857\n",
      "Epoch 148/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.3843 - acc: 0.5862 - val_loss: 0.5091 - val_acc: 0.4925\n",
      "Epoch 149/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3883 - acc: 0.5729 - val_loss: 0.5164 - val_acc: 0.4884\n",
      "Epoch 150/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3899 - acc: 0.5770 - val_loss: 0.5084 - val_acc: 0.4870\n",
      "Epoch 151/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3858 - acc: 0.5804 - val_loss: 0.5115 - val_acc: 0.4884\n",
      "Epoch 152/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3836 - acc: 0.5848 - val_loss: 0.5128 - val_acc: 0.4939\n",
      "Epoch 153/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3749 - acc: 0.5924 - val_loss: 0.5100 - val_acc: 0.4911\n",
      "Epoch 154/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3799 - acc: 0.5862 - val_loss: 0.5101 - val_acc: 0.4966\n",
      "Epoch 155/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3751 - acc: 0.6016 - val_loss: 0.5106 - val_acc: 0.4898\n",
      "Epoch 156/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3789 - acc: 0.5975 - val_loss: 0.5095 - val_acc: 0.4993\n",
      "Epoch 157/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.3786 - acc: 0.5896 - val_loss: 0.5070 - val_acc: 0.4911\n",
      "Epoch 158/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3755 - acc: 0.5883 - val_loss: 0.5072 - val_acc: 0.4952\n",
      "Epoch 159/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3724 - acc: 0.6060 - val_loss: 0.5108 - val_acc: 0.5020\n",
      "Epoch 160/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3757 - acc: 0.5930 - val_loss: 0.5066 - val_acc: 0.5061\n",
      "Epoch 161/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3707 - acc: 0.6094 - val_loss: 0.5014 - val_acc: 0.5034\n",
      "Epoch 162/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3727 - acc: 0.6094 - val_loss: 0.5014 - val_acc: 0.4966\n",
      "Epoch 163/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3740 - acc: 0.5985 - val_loss: 0.4991 - val_acc: 0.4993\n",
      "Epoch 164/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3703 - acc: 0.6104 - val_loss: 0.5021 - val_acc: 0.5007\n",
      "Epoch 165/200\n",
      "92/92 [==============================] - 95s 1s/step - loss: 0.3716 - acc: 0.6029 - val_loss: 0.5015 - val_acc: 0.5102\n",
      "Epoch 166/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3650 - acc: 0.6241 - val_loss: 0.5031 - val_acc: 0.5061\n",
      "Epoch 167/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3619 - acc: 0.6289 - val_loss: 0.5012 - val_acc: 0.4993\n",
      "Epoch 168/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3623 - acc: 0.6169 - val_loss: 0.5013 - val_acc: 0.5116\n",
      "Epoch 169/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3598 - acc: 0.6265 - val_loss: 0.4995 - val_acc: 0.4993\n",
      "Epoch 170/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3616 - acc: 0.6152 - val_loss: 0.5031 - val_acc: 0.5048\n",
      "Epoch 171/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3593 - acc: 0.6217 - val_loss: 0.5005 - val_acc: 0.5048\n",
      "Epoch 172/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3589 - acc: 0.6337 - val_loss: 0.4982 - val_acc: 0.5061\n",
      "Epoch 173/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3586 - acc: 0.6210 - val_loss: 0.4968 - val_acc: 0.5089\n",
      "Epoch 174/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3550 - acc: 0.6313 - val_loss: 0.5032 - val_acc: 0.5061\n",
      "Epoch 175/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3526 - acc: 0.6320 - val_loss: 0.4946 - val_acc: 0.5048\n",
      "Epoch 176/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.3567 - acc: 0.6275 - val_loss: 0.4990 - val_acc: 0.5102\n",
      "Epoch 177/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.3579 - acc: 0.6244 - val_loss: 0.4908 - val_acc: 0.5130\n",
      "Epoch 178/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3476 - acc: 0.6432 - val_loss: 0.4891 - val_acc: 0.5075\n",
      "Epoch 179/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3489 - acc: 0.6357 - val_loss: 0.4923 - val_acc: 0.5034\n",
      "Epoch 180/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.3461 - acc: 0.6497 - val_loss: 0.4956 - val_acc: 0.5075\n",
      "Epoch 181/200\n",
      "92/92 [==============================] - 96s 1s/step - loss: 0.3526 - acc: 0.6262 - val_loss: 0.4878 - val_acc: 0.5061\n",
      "Epoch 182/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3518 - acc: 0.6279 - val_loss: 0.4919 - val_acc: 0.5089\n",
      "Epoch 183/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3486 - acc: 0.6374 - val_loss: 0.4972 - val_acc: 0.5089\n",
      "Epoch 184/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3425 - acc: 0.6395 - val_loss: 0.4879 - val_acc: 0.5130\n",
      "Epoch 185/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3453 - acc: 0.6381 - val_loss: 0.4822 - val_acc: 0.5075\n",
      "Epoch 186/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3424 - acc: 0.6422 - val_loss: 0.4869 - val_acc: 0.5061\n",
      "Epoch 187/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3447 - acc: 0.6381 - val_loss: 0.4873 - val_acc: 0.5102\n",
      "Epoch 188/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3433 - acc: 0.6388 - val_loss: 0.4860 - val_acc: 0.5143\n",
      "Epoch 189/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3415 - acc: 0.6432 - val_loss: 0.4858 - val_acc: 0.5089\n",
      "Epoch 190/200\n",
      "92/92 [==============================] - 95s 1s/step - loss: 0.3387 - acc: 0.6442 - val_loss: 0.4851 - val_acc: 0.5143\n",
      "Epoch 191/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3439 - acc: 0.6419 - val_loss: 0.4933 - val_acc: 0.5130\n",
      "Epoch 192/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3381 - acc: 0.6395 - val_loss: 0.4830 - val_acc: 0.5130\n",
      "Epoch 193/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3302 - acc: 0.6675 - val_loss: 0.4862 - val_acc: 0.5102\n",
      "Epoch 194/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3397 - acc: 0.6453 - val_loss: 0.4888 - val_acc: 0.5075\n",
      "Epoch 195/200\n",
      "92/92 [==============================] - 93s 1s/step - loss: 0.3314 - acc: 0.6620 - val_loss: 0.4868 - val_acc: 0.5102\n",
      "Epoch 196/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.3356 - acc: 0.6518 - val_loss: 0.4808 - val_acc: 0.5130\n",
      "Epoch 197/200\n",
      "92/92 [==============================] - 92s 999ms/step - loss: 0.3315 - acc: 0.6620 - val_loss: 0.4799 - val_acc: 0.5102\n",
      "Epoch 198/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3293 - acc: 0.6569 - val_loss: 0.4809 - val_acc: 0.5075\n",
      "Epoch 199/200\n",
      "92/92 [==============================] - 92s 1s/step - loss: 0.3267 - acc: 0.6647 - val_loss: 0.4865 - val_acc: 0.5102\n",
      "Epoch 200/200\n",
      "92/92 [==============================] - 92s 1000ms/step - loss: 0.3272 - acc: 0.6661 - val_loss: 0.4777 - val_acc: 0.5143\n"
     ]
    }
   ],
   "source": [
    "# Compile and Execute the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_batches,\n",
    "                    epochs=200,\n",
    "                    validation_data=test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 7s 300ms/step - loss: 0.4777 - acc: 0.5143\n",
      "Accuracy:  0.5143246650695801\n"
     ]
    }
   ],
   "source": [
    "# Show the final accuracy of the model\n",
    "acc = model.evaluate(test_batches, verbose=1)\n",
    "print(\"Accuracy: \", acc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f4bcf6b7f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the graphs of how the solution converges: Accuracy and Loss for the training and test data\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='center left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='center right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 predictions: ['Proliferate_DR', 'No_DR', 'Proliferate_DR', 'Severe', 'Proliferate_DR', 'Proliferate_DR', 'No_DR', 'Severe', 'No_DR', 'Severe']\n"
     ]
    }
   ],
   "source": [
    "#  Predict the label of the test_images\n",
    "pred = model.predict(test_batches)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "\n",
    "# Map the label\n",
    "labels = (train_batches.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "pred = [labels[k] for k in pred]\n",
    "\n",
    "# Display the result\n",
    "print(f'The first 10 predictions: {pred[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"Retinopathy_model_trained_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "retinopathy_model = load_model(\"Retinopathy_model_trained_200.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 - 7s - loss: 0.4777 - acc: 0.5143\n",
      "Normal Neural Network - Loss: 0.47771576046943665, Accuracy: 0.5143246650695801\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = retinopathy_model.evaluate(\n",
    "    test_batches, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame.from_dict(history.history, orient=\"index\")\n",
    "df.to_csv(\"200_history_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.758151</td>\n",
       "      <td>0.723468</td>\n",
       "      <td>0.703276</td>\n",
       "      <td>0.687834</td>\n",
       "      <td>0.676530</td>\n",
       "      <td>0.667212</td>\n",
       "      <td>0.658265</td>\n",
       "      <td>0.652577</td>\n",
       "      <td>0.642991</td>\n",
       "      <td>0.636487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343898</td>\n",
       "      <td>0.338126</td>\n",
       "      <td>0.330218</td>\n",
       "      <td>0.339683</td>\n",
       "      <td>0.331363</td>\n",
       "      <td>0.335606</td>\n",
       "      <td>0.331470</td>\n",
       "      <td>0.329284</td>\n",
       "      <td>0.326747</td>\n",
       "      <td>0.327194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>0.113691</td>\n",
       "      <td>0.114374</td>\n",
       "      <td>0.121885</td>\n",
       "      <td>0.124957</td>\n",
       "      <td>0.132127</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.133834</td>\n",
       "      <td>0.139297</td>\n",
       "      <td>0.141345</td>\n",
       "      <td>0.149198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641857</td>\n",
       "      <td>0.639467</td>\n",
       "      <td>0.667463</td>\n",
       "      <td>0.645271</td>\n",
       "      <td>0.662001</td>\n",
       "      <td>0.651758</td>\n",
       "      <td>0.662001</td>\n",
       "      <td>0.656879</td>\n",
       "      <td>0.664732</td>\n",
       "      <td>0.666098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_loss</th>\n",
       "      <td>0.692562</td>\n",
       "      <td>0.684705</td>\n",
       "      <td>0.688095</td>\n",
       "      <td>0.699078</td>\n",
       "      <td>0.716522</td>\n",
       "      <td>0.714211</td>\n",
       "      <td>0.703471</td>\n",
       "      <td>0.694537</td>\n",
       "      <td>0.688101</td>\n",
       "      <td>0.681864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493317</td>\n",
       "      <td>0.482999</td>\n",
       "      <td>0.486228</td>\n",
       "      <td>0.488793</td>\n",
       "      <td>0.486829</td>\n",
       "      <td>0.480806</td>\n",
       "      <td>0.479899</td>\n",
       "      <td>0.480870</td>\n",
       "      <td>0.486454</td>\n",
       "      <td>0.477716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_acc</th>\n",
       "      <td>0.076398</td>\n",
       "      <td>0.076398</td>\n",
       "      <td>0.081855</td>\n",
       "      <td>0.098226</td>\n",
       "      <td>0.111869</td>\n",
       "      <td>0.136426</td>\n",
       "      <td>0.177353</td>\n",
       "      <td>0.178718</td>\n",
       "      <td>0.190996</td>\n",
       "      <td>0.169168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512960</td>\n",
       "      <td>0.512960</td>\n",
       "      <td>0.510232</td>\n",
       "      <td>0.507503</td>\n",
       "      <td>0.510232</td>\n",
       "      <td>0.512960</td>\n",
       "      <td>0.510232</td>\n",
       "      <td>0.507503</td>\n",
       "      <td>0.510232</td>\n",
       "      <td>0.514325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows  200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5  \\\n",
       "loss      0.758151  0.723468  0.703276  0.687834  0.676530  0.667212   \n",
       "acc       0.113691  0.114374  0.121885  0.124957  0.132127  0.128713   \n",
       "val_loss  0.692562  0.684705  0.688095  0.699078  0.716522  0.714211   \n",
       "val_acc   0.076398  0.076398  0.081855  0.098226  0.111869  0.136426   \n",
       "\n",
       "                 6         7         8         9  ...       190       191  \\\n",
       "loss      0.658265  0.652577  0.642991  0.636487  ...  0.343898  0.338126   \n",
       "acc       0.133834  0.139297  0.141345  0.149198  ...  0.641857  0.639467   \n",
       "val_loss  0.703471  0.694537  0.688101  0.681864  ...  0.493317  0.482999   \n",
       "val_acc   0.177353  0.178718  0.190996  0.169168  ...  0.512960  0.512960   \n",
       "\n",
       "               192       193       194       195       196       197  \\\n",
       "loss      0.330218  0.339683  0.331363  0.335606  0.331470  0.329284   \n",
       "acc       0.667463  0.645271  0.662001  0.651758  0.662001  0.656879   \n",
       "val_loss  0.486228  0.488793  0.486829  0.480806  0.479899  0.480870   \n",
       "val_acc   0.510232  0.507503  0.510232  0.512960  0.510232  0.507503   \n",
       "\n",
       "               198       199  \n",
       "loss      0.326747  0.327194  \n",
       "acc       0.664732  0.666098  \n",
       "val_loss  0.486454  0.477716  \n",
       "val_acc   0.510232  0.514325  \n",
       "\n",
       "[4 rows x 200 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload_history=pd.read_csv(\"200_history_data.csv\", index_col=0)\n",
    "reload_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_history = reload_history.to_dict(\"split\")\n",
    "\n",
    "new_history = dict(zip(new_history[\"index\"], new_history[\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"200_pred.csv\", 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "     wr.writerow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageDataGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-93d46ebbe1d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_x_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrescale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtest_x_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrescale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "train_path = 'train'\n",
    "test_path = 'test'\n",
    "\n",
    "train_x_batches = ImageDataGenerator(rescale = 1./255).flow_from_directory(train_path, target_size=(224,224), shuffle = False)\n",
    "test_x_batches = ImageDataGenerator(rescale = 1./255).flow_from_directory(test_path, target_size=(224,224), shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
